{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#!/usr/bin/python\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import dateparser\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for log file reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reader(fin):\n",
    "    \"\"\"\n",
    "    Read stoRM's log file and transform into a list of lines (str)\n",
    "    \n",
    "    Recive path (string) to unstructured log file\n",
    "    Return a list containing where each element is a log's line\n",
    "    \"\"\"\n",
    "    listed_log = []\n",
    "    \n",
    "    input_file = open(fin,\"r\")\n",
    "    for line in input_file:\n",
    "        listed_log.append(line.strip())\n",
    "    input_file.close()\n",
    "    \n",
    "    return listed_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reader_small(fin,start,end):\n",
    "    \"\"\"\n",
    "    Read stoRM's log file and transform into a list of lines (str)\n",
    "    \n",
    "    Recive path (string) to unstructured log file\n",
    "    Return a list containing where each element is a log's line\n",
    "    \"\"\"\n",
    "    listed_log = []\n",
    "    \n",
    "    input_file = open(fin,\"r\")\n",
    "    i = start\n",
    "    for line in input_file:\n",
    "        listed_log.append(line.strip())\n",
    "        i+=1\n",
    "        if i == end:\n",
    "            break\n",
    "    input_file.close()\n",
    "    \n",
    "    return listed_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Function for separating the log fields and trasform the log into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_tabler(listed_log):\n",
    "    \"\"\"\n",
    "    Transform a log's list of lines (str) in a dictionary\n",
    "    \n",
    "    Recive a list containing where each element is a (stoRM) log's line\n",
    "    Return a table (dict) where each key is a log's column\n",
    "    \n",
    "    BUG: NONE type reading not correct.\n",
    "    \"\"\"\n",
    "    date, time_stamp, thread, tipe, token, message = [], [], [], [], [], []\n",
    "    it = 0\n",
    "    total = len(listed_log)\n",
    "    for line in listed_log:\n",
    "        date.append(line[:18])\n",
    "        time_stamp.append(dateparser.storm_dtpars(line[:18]))\n",
    "        thread.append(line.split(\" \",4)[3])\n",
    "        tipe.append(line.split(\" \",7)[6])\n",
    "        token.append(line.split(\"[\",1)[1].split(\"]\",1)[0])\n",
    "        message.append(line.split(\":\",3)[3].rstrip().lstrip())\n",
    "        #if it%100000 == 0 :\n",
    "        #    print \" parsed line {0} of {1} lines\".format(it,total)\n",
    "        #if it == total:\n",
    "        #    print \"END\"\n",
    "        #it+=1\n",
    "        \n",
    "    log_table = {'date':date, 'timestamp':time_stamp, 'thread':thread,\\\n",
    "                 'type':tipe, 'request_id':token, 'message':message}\n",
    "    return log_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to transform the log into a .csv/.msg; automaticly use the previous declareted functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csver(fin,fout):\n",
    "    \"\"\"\n",
    "    Transform a log (dictionary) in .csv\n",
    "    \n",
    "    Recive a table (dict) where each key is a log's column\n",
    "           a string of the filepath output and file name\n",
    "    Return None\n",
    "    Produce a structured .csv file of a stoRM log file\n",
    "    \"\"\"\n",
    "    log_table = (log_tabler(log_reader(fin)))\n",
    "    dataf = pd.DataFrame.from_dict(log_table)\n",
    "    #P: find out columns order\n",
    "    #print dataf.columns.tolist()\n",
    "    \n",
    "    #P: riarrange columns order\n",
    "    cols =['date', 'timestamp', 'type','thread', 'request_id','message']\n",
    "    dataf = dataf[cols]\n",
    "    \n",
    "    #print dataf.describe()\n",
    "    dataf.to_csv(fout + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csver_small(fin,fout,start, end):\n",
    "    \"\"\"\n",
    "    Transform a log slice (dctionary) in .csv\n",
    "    \n",
    "    Recive a table (dict) where each key is a log's column\n",
    "           a string of the filepath output and file name\n",
    "    Return None\n",
    "    Produce a structured .csv file of a stoRM log file\n",
    "    \"\"\"\n",
    "    log_table = (log_tabler(log_reader_small(fin,start,end)))\n",
    "    dataf = pd.DataFrame.from_dict(log_table)\n",
    "    #P: find out columns order\n",
    "    #print dataf.columns.tolist()\n",
    "    \n",
    "    #P: riarrange columns order\n",
    "    cols =['date', 'timestamp', 'type','thread', 'request_id','message']\n",
    "    dataf = dataf[cols]\n",
    "    \n",
    "    #print dataf.describe()\n",
    "    dataf.to_csv(fout + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msger(fin,fout):\n",
    "    \"\"\"\n",
    "    Transform a log's dictionary in msgpack \n",
    "    \n",
    "    Recive a table (dict) where each key is a log's column\n",
    "           a string of the filepath output and file name\n",
    "    Return None\n",
    "    Produce a msgpack file of a stoRM log file\n",
    "    \"\"\"\n",
    "    log_table = (log_tabler(log_reader(fin)))\n",
    "    dataf = pd.DataFrame.from_dict(log_table)\n",
    "    cols = ['date', 'timestamp', 'type','thread', 'request_id','message']\n",
    "    dataf = dataf[cols]\n",
    "    dataf.to_msgpack(fout + '.msg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msger_small(fin,fout,start,end):\n",
    "    \"\"\"\n",
    "    Transform a log's slice dictionary in msgpack \n",
    "    \n",
    "    Recive a table (dict) where each key is a log's column\n",
    "           a string of the filepath output and file name\n",
    "    Return None\n",
    "    Produce a msgpack file of a stoRM log file\n",
    "    \"\"\"\n",
    "    log_table = (log_tabler(log_reader_small(fin,start,end)))\n",
    "    dataf = pd.DataFrame.from_dict(log_table)\n",
    "    cols = ['date', 'timestamp', 'type','thread', 'request_id','message']\n",
    "    dataf = dataf[cols]\n",
    "    dataf.to_msgpack(fout + '_small' + '.msg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import the log pre-parsed by kibana and return it adding a timestamp (s) and uniforming the fields to the standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_kibana_parser(fin, fout):\n",
    "    \"\"\"\n",
    "    re-parse csv logs parsed by kibana adding timestamp (seconds) and renaming columns\n",
    "    \"\"\"\n",
    "    storm_df = pd.read_csv(fin)\n",
    "    lista_timest = map(dateparser.kibana_dtpars,list(storm_df['@timestamp'].values))\n",
    "    storm_df['timestamp'] = pd.Series(lista_timest).values\n",
    "    storm_df = storm_df.rename(index=str, columns={'@timestamp':'date ISO-8601','thread':'thread','status':'type','token':'request_id','text':'message'})\n",
    "    cols = ['date ISO-8601', 'timestamp', 'type','thread', 'request_id','message']\n",
    "    storm_df = storm_df[cols]\n",
    "    storm_df.to_csv(fout + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for parsing the message body, works on DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funzb(line):\n",
    "    \"\"\"\n",
    "    Author      : Luca Giommi <luca dot giommi2 AT studio dot unibo dot it>\n",
    "    \"\"\"\n",
    "    startup=''\n",
    "    ResponseHandler=''\n",
    "    request=re.search(r'((?<=request \\')|(?<=Request \\')|(?<=REQUEST \\')|(?<=Request: )|(?<=process_request : )).*?(?=\\'|\\.| from)', line, re.M|re.I)\n",
    "    DN = re.search(r'(((?<=Client DN=\\').*?(?=\\'))|((?<=Client DN: ).*?(?= surl\\(s\\)))|((?<=Client DN: ).*?(?=\\.)))', line, re.M|re.I)\n",
    "    ip = re.search(r'(::(ffff(:0{1,4}){0,1}:){0,1}((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])|((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)?)', line, re.M|re.I)\n",
    "    token = re.search(r'(((?<=# Requested token \\').*?(?=\\'))|((?<=# Produced request token: \\').*?(?=\\'))|(((?<=token: )|(?<=token description: )).*))', line, re.M|re.I)\n",
    "    num = re.search(r'((?<=\\' on \\').*?(?=\\')|(?<=# Requested \\').*?(?=\\'))', line, re.M|re.I)\n",
    "    surl = re.search(r'(((?<=SURL\\(s\\): \\').*?((?=\\')|(.*)))|((?<=surl\\(s\\): ).*?(?= token))|((?<=surl\\(s\\): ).*))', line, re.M|re.I)\n",
    "    result = re.search(r'(((?<=is \\').*?(?=\\'))|((?<=__process_file_request\\<\\> : ).*))', line, re.M|re.I)\n",
    "    \n",
    "    if re.search(r'((logConfiguration : )|(initSoap : )|(main : ))', line, re.M|re.I):\n",
    "        startup=line\n",
    "    if re.search(r'(rpcResponseHandler_AbortFiles)', line, re.M|re.I):\n",
    "        ResponseHandler=line\n",
    "    \n",
    "    result={'Request':request,'DN':DN,'requested_token':token,'num':num,'surl':surl,'result':result, 'ip':ip, 'startup':startup}\n",
    "    for key, value in result.iteritems():\n",
    "        if key=='startup':\n",
    "            result[key]=startup\n",
    "            continue\n",
    "        if key=='result' and ResponseHandler != '':\n",
    "            result[key]=ResponseHandler\n",
    "            continue\n",
    "        if value:\n",
    "            result[key]=value.group()\n",
    "            if key=='surl':\n",
    "                result[key]=result[key].replace(\"\\'\",\"\")\n",
    "                result[key]=result[key].split()\n",
    "        else:\n",
    "            result[key]=''\n",
    "        \n",
    "    if len(result['surl'])>4:\n",
    "        result['surl'][3]=result['surl'][4]\n",
    "        result['surl']=result['surl'][:4]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_parser(storm_df):\n",
    "    \"\"\"\n",
    "    Recive stoRM dataframe with message NOT parsed ad use funzb to return a new DF with parsed message.\n",
    "    Takes about 15 min to parse 4mln lines log. \n",
    "    \n",
    "    BUG: inizializza anche campi del dizionario, ergo del DF successivo, che potremmo non trovare poi.\n",
    "    \n",
    "    \"\"\"\n",
    "    inizio = {'DN': '', 'Request': '', 'ip': '', 'num': '', 'requested_token': '', 'result': '', 'startup': '', 'surl': ''}\n",
    "    lista = list(storm_df['message'])\n",
    "    listaa = map(funzb,lista)\n",
    "    finale={}\n",
    "    for key in inizio.iterkeys():\n",
    "        finale[key]=list(finale[key] for finale in listaa)\n",
    "    finale = pd.DataFrame.from_dict(finale)\n",
    "    finalone= pd.concat([storm_df, finale], axis=1)\n",
    "    return finalone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import a log .csv into a pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dframer(fin):\n",
    "    \"\"\"\n",
    "    simple function to read log's csv and return DF\n",
    "    \"\"\"\n",
    "    dataf = pd.read_csv(fin,index_col=None)\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
