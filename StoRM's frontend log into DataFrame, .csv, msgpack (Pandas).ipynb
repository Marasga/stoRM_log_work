{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#!/usr/bin/python\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reader(fin):\n",
    "    \"\"\"\n",
    "    Read stoRM's log file and transform into a list of lines (str)\n",
    "    \n",
    "    Recive path (string) to unstructured log file\n",
    "    Return a list containing where each element is a log's line\n",
    "    \"\"\"\n",
    "    listed_log = []\n",
    "    \n",
    "    input_file = open(fin,\"r\")\n",
    "    i = start\n",
    "    for line in input_file:\n",
    "        listed_log.append(line.strip())\n",
    "        if i == end:\n",
    "            break\n",
    "    input_file.close()\n",
    "    \n",
    "    return listed_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reader_small(fin,start,end):\n",
    "    \"\"\"\n",
    "    Read stoRM's log file and transform into a list of lines (str)\n",
    "    \n",
    "    Recive path (string) to unstructured log file\n",
    "    Return a list containing where each element is a log's line\n",
    "    \"\"\"\n",
    "    listed_log = []\n",
    "    \n",
    "    input_file = open(fin,\"r\")\n",
    "    i = start\n",
    "    for line in input_file:\n",
    "        listed_log.append(line.strip())\n",
    "        i+=1\n",
    "        if i == end:\n",
    "            break\n",
    "    input_file.close()\n",
    "    \n",
    "    return listed_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_tabler(listed_log):\n",
    "    \"\"\"\n",
    "    Transform a log's list of lines (str) in a dictionary\n",
    "    \n",
    "    Recive a list containing where each element is a (stoRM) log's line\n",
    "    Return a table (dict) where each key is a log's column\n",
    "    \"\"\"\n",
    "    #timestamp is yet to be finished\n",
    "    date, time_stamp, thread, tipe, token, message = [], [], [], [], [], []\n",
    "    it = 0\n",
    "    total = len(listed_log)\n",
    "    for line in listed_log:\n",
    "        date.append(line[:18])\n",
    "        time_stamp.append(dateparser.storm_dtpars(line[:18]))\n",
    "        thread.append(line.split(\" \",4)[3])\n",
    "        tipe.append(line.split(\" \",7)[6])\n",
    "        token.append(line.split(\"[\",1)[1].split(\"]\",1)[0])\n",
    "        message.append(line.split(\":\",3)[3].rstrip().lstrip())\n",
    "        if it%100000 == 0 :\n",
    "            print \" parsed line {0} of {1} lines\".format(it,total)\n",
    "        if it == total:\n",
    "            print \"END\"\n",
    "        it+=1\n",
    "        \n",
    "    log_table = {'DATE':date, 'TIMESTAMP':time_stamp, 'THREAD':thread,\\\n",
    "                 'TYPE':tipe, 'TOKEN':token, 'MESSAGE':message}\n",
    "    return log_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csver(fin,fout):\n",
    "    \"\"\"\n",
    "    Transform a log (dictionary) in .csv\n",
    "    \n",
    "    Recive a table (dict) where each key is a log's column\n",
    "           a string of the filepath output and file name\n",
    "    Return None\n",
    "    Produce a structured .csv file of a stoRM log file\n",
    "    \"\"\"\n",
    "    log_table = (log_tabler(log_reader(fin)))\n",
    "    dataf = pd.DataFrame.from_dict(log_table)\n",
    "    #P: find out columns order\n",
    "    #print dataf.columns.tolist()\n",
    "    \n",
    "    #P: riarrange columns order\n",
    "    cols =['DATE', 'TIMESTAMP', 'TYPE','THREAD', 'TOKEN','MESSAGE']\n",
    "    dataf = dataf[cols]\n",
    "    \n",
    "    #print dataf.describe()\n",
    "    dataf.to_csv(fout + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csver_small(fin,fout,start, end):\n",
    "    \"\"\"\n",
    "    Transform a log slice (dctionary) in .csv\n",
    "    \n",
    "    Recive a table (dict) where each key is a log's column\n",
    "           a string of the filepath output and file name\n",
    "    Return None\n",
    "    Produce a structured .csv file of a stoRM log file\n",
    "    \"\"\"\n",
    "    log_table = (log_tabler(log_reader_small(fin,start,end)))\n",
    "    dataf = pd.DataFrame.from_dict(log_table)\n",
    "    #P: find out columns order\n",
    "    #print dataf.columns.tolist()\n",
    "    \n",
    "    #P: riarrange columns order\n",
    "    cols =['DATE', 'TIMESTAMP', 'TYPE','THREAD', 'TOKEN','MESSAGE']\n",
    "    dataf = dataf[cols]\n",
    "    \n",
    "    #print dataf.describe()\n",
    "    dataf.to_csv(fout + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msger(fin,fout):\n",
    "    \"\"\"\n",
    "    Transform a log's dictionary in msgpack \n",
    "    \n",
    "    Recive a table (dict) where each key is a log's column\n",
    "           a string of the filepath output and file name\n",
    "    Return None\n",
    "    Produce a msgpack file of a stoRM log file\n",
    "    \"\"\"\n",
    "    log_table = (log_tabler(log_reader(fin)))\n",
    "    dataf = pd.DataFrame.from_dict(log_table)\n",
    "    cols = ['DATE', 'TIMESTAMP', 'TYPE','THREAD', 'TOKEN','MESSAGE']\n",
    "    dataf = dataf[cols]\n",
    "    dataf.to_msgpack(fout + '.msg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msger_small(fin,fout,start,end):\n",
    "    \"\"\"\n",
    "    Transform a log's slice dictionary in msgpack \n",
    "    \n",
    "    Recive a table (dict) where each key is a log's column\n",
    "           a string of the filepath output and file name\n",
    "    Return None\n",
    "    Produce a msgpack file of a stoRM log file\n",
    "    \"\"\"\n",
    "    log_table = (log_tabler(log_reader_small(fin,start,end)))\n",
    "    dataf = pd.DataFrame.from_dict(log_table)\n",
    "    cols = ['DATE', 'TIMESTAMP', 'TYPE','THREAD', 'TOKEN','MESSAGE']\n",
    "    dataf = dataf[cols]\n",
    "    dataf.to_msgpack(fout + '_small' + '.msg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " parsed line 0 of 10000 lines\n",
      "CPU times: user 440 ms, sys: 7.91 ms, total: 448 ms\n",
      "Wall time: 451 ms\n"
     ]
    }
   ],
   "source": [
    "%time csver_small(\"log_non_strutturati/storm-frontend-server.log-20180901\",\"test\",start=0,end=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(16ML lines log)<br>\n",
    "csver => CPU times: user 11min 48s, sys: 17.7 s, total: 12min 6s Wall time: 12min 10s <br> msper => CPU times: user 10min 44s, sys: 18.7 s, total: 11min 2s Wall time: 11min 10s\n",
    "<br>\n",
    "<span style='color:red'> Un grosso gap da risolvere è che comunque anche per creare log strutturati parziali con il codice fatto in questo modo noi ci parsiamo prima tutto il codice, **anche perche ci giochiamo facile 14gb di ram**, il che non è esatamente una cosa efficente. In un momento del tempo cambia questa cosa. </span>\n",
    "<span style='color:green'> **FATTO** </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test per importazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msg_to_df(file_path):\n",
    "    storm_df = pd.read_msgpack(file_path)\n",
    "    return storm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_df(file_path):\n",
    "    storm_df = pd.read_csv(file_path)\n",
    "    return storm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time a = msg_to_df(\"/home/gabriele/Documenti/storm-frontend-server.log-20180901.msg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time b = csv_to_df(\"/home/gabriele/Documenti/storm-frontend-server.log-20180901.csv\",d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
