{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#!/usr/bin/python\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import dateparser\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reader(fin):\n",
    "    \"\"\"\n",
    "    Read stoRM's log file and transform into a list of lines (str)\n",
    "    \n",
    "    Recive path (string) to unstructured log file\n",
    "    Return a list containing where each element is a log's line\n",
    "    \"\"\"\n",
    "    listed_log = []\n",
    "    \n",
    "    input_file = open(fin,\"r\")\n",
    "    for line in input_file:\n",
    "        listed_log.append(line.strip())\n",
    "    input_file.close()\n",
    "    \n",
    "    return listed_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reader_small(fin,start,end):\n",
    "    \"\"\"\n",
    "    Read stoRM's log file and transform into a list of lines (str)\n",
    "    \n",
    "    Recive path (string) to unstructured log file\n",
    "    Return a list containing where each element is a log's line\n",
    "    \"\"\"\n",
    "    listed_log = []\n",
    "    \n",
    "    input_file = open(fin,\"r\")\n",
    "    i = start\n",
    "    for line in input_file:\n",
    "        listed_log.append(line.strip())\n",
    "        i+=1\n",
    "        if i == end:\n",
    "            break\n",
    "    input_file.close()\n",
    "    \n",
    "    return listed_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_tabler(listed_log):\n",
    "    \"\"\"\n",
    "    Transform a log's list of lines (str) in a dictionary\n",
    "    \n",
    "    Recive a list containing where each element is a (stoRM) log's line\n",
    "    Return a table (dict) where each key is a log's column\n",
    "    \"\"\"\n",
    "    #timestamp is yet to be finished\n",
    "    date, time_stamp, thread, tipe, token, message = [], [], [], [], [], []\n",
    "    it = 0\n",
    "    total = len(listed_log)\n",
    "    for line in listed_log:\n",
    "        date.append(line[:18])\n",
    "        time_stamp.append(dateparser.storm_dtpars(line[:18]))\n",
    "        thread.append(line.split(\" \",4)[3])\n",
    "        tipe.append(line.split(\" \",7)[6])\n",
    "        token.append(line.split(\"[\",1)[1].split(\"]\",1)[0])\n",
    "        message.append(line.split(\":\",3)[3].rstrip().lstrip())\n",
    "        #if it%100000 == 0 :\n",
    "        #    print \" parsed line {0} of {1} lines\".format(it,total)\n",
    "        #if it == total:\n",
    "        #    print \"END\"\n",
    "        #it+=1\n",
    "        \n",
    "    log_table = {'date':date, 'timestamp':time_stamp, 'thread':thread,\\\n",
    "                 'type':tipe, 'request_id':token, 'message':message}\n",
    "    return log_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dframer(fin):\n",
    "    \"\"\"\n",
    "    simple function to read log's csv and return DF\n",
    "    \"\"\"\n",
    "    dataf = pd.read_csv(fin,index_col=None)\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csver(fin,fout):\n",
    "    \"\"\"\n",
    "    Transform a log (dictionary) in .csv\n",
    "    \n",
    "    Recive a table (dict) where each key is a log's column\n",
    "           a string of the filepath output and file name\n",
    "    Return None\n",
    "    Produce a structured .csv file of a stoRM log file\n",
    "    \"\"\"\n",
    "    log_table = (log_tabler(log_reader(fin)))\n",
    "    dataf = pd.DataFrame.from_dict(log_table)\n",
    "    #P: find out columns order\n",
    "    #print dataf.columns.tolist()\n",
    "    \n",
    "    #P: riarrange columns order\n",
    "    cols =['date', 'timestamp', 'type','thread', 'request_id','message']\n",
    "    dataf = dataf[cols]\n",
    "    \n",
    "    #print dataf.describe()\n",
    "    dataf.to_csv(fout + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csver_small(fin,fout,start, end):\n",
    "    \"\"\"\n",
    "    Transform a log slice (dctionary) in .csv\n",
    "    \n",
    "    Recive a table (dict) where each key is a log's column\n",
    "           a string of the filepath output and file name\n",
    "    Return None\n",
    "    Produce a structured .csv file of a stoRM log file\n",
    "    \"\"\"\n",
    "    log_table = (log_tabler(log_reader_small(fin,start,end)))\n",
    "    dataf = pd.DataFrame.from_dict(log_table)\n",
    "    #P: find out columns order\n",
    "    #print dataf.columns.tolist()\n",
    "    \n",
    "    #P: riarrange columns order\n",
    "    cols =['date', 'timestamp', 'type','thread', 'request_id','message']\n",
    "    dataf = dataf[cols]\n",
    "    \n",
    "    #print dataf.describe()\n",
    "    dataf.to_csv(fout + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msger(fin,fout):\n",
    "    \"\"\"\n",
    "    Transform a log's dictionary in msgpack \n",
    "    \n",
    "    Recive a table (dict) where each key is a log's column\n",
    "           a string of the filepath output and file name\n",
    "    Return None\n",
    "    Produce a msgpack file of a stoRM log file\n",
    "    \"\"\"\n",
    "    log_table = (log_tabler(log_reader(fin)))\n",
    "    dataf = pd.DataFrame.from_dict(log_table)\n",
    "    cols = ['date', 'timestamp', 'type','thread', 'request_id','message']\n",
    "    dataf = dataf[cols]\n",
    "    dataf.to_msgpack(fout + '.msg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msger_small(fin,fout,start,end):\n",
    "    \"\"\"\n",
    "    Transform a log's slice dictionary in msgpack \n",
    "    \n",
    "    Recive a table (dict) where each key is a log's column\n",
    "           a string of the filepath output and file name\n",
    "    Return None\n",
    "    Produce a msgpack file of a stoRM log file\n",
    "    \"\"\"\n",
    "    log_table = (log_tabler(log_reader_small(fin,start,end)))\n",
    "    dataf = pd.DataFrame.from_dict(log_table)\n",
    "    cols = ['date', 'timestamp', 'type','thread', 'request_id','message']\n",
    "    dataf = dataf[cols]\n",
    "    dataf.to_msgpack(fout + '_small' + '.msg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_kibana_parser(fin, fout):\n",
    "    \"\"\"\n",
    "    re-parse csv logs parsed by kibana adding timestamp (seconds) and renaming columns\n",
    "    \"\"\"\n",
    "    storm_df = pd.read_csv(fin)\n",
    "    lista_timest = map(dateparser.kibana_dtpars,list(storm_df['@timestamp'].values))\n",
    "    storm_df['timestamp'] = pd.Series(lista_timest).values\n",
    "    storm_df = storm_df.rename(index=str, columns={'@timestamp':'date ISO-8601','thread':'thread','status':'type','token':'request_id','text':'message'})\n",
    "    cols = ['date ISO-8601', 'timestamp', 'type','thread', 'request_id','message']\n",
    "    storm_df = storm_df[cols]\n",
    "    storm_df.to_csv(fout + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funzb(line):\n",
    "    \"\"\"\n",
    "    author: Luca Giommi\n",
    "    \"\"\"\n",
    "    startup=''\n",
    "    ResponseHandler=''\n",
    "    request=re.search(r'((?<=request \\')|(?<=Request \\')|(?<=REQUEST \\')|(?<=Request: )|(?<=process_request : )).*?(?=\\'|\\.| from)', line, re.M|re.I)\n",
    "    DN = re.search(r'(((?<=Client DN=\\').*?(?=\\'))|((?<=Client DN: ).*?(?= surl\\(s\\)))|((?<=Client DN: ).*?(?=\\.)))', line, re.M|re.I)\n",
    "    ip = re.search(r'(::(ffff(:0{1,4}){0,1}:){0,1}((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])|((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)?)', line, re.M|re.I)\n",
    "    token = re.search(r'(((?<=# Requested token \\').*?(?=\\'))|((?<=# Produced request token: \\').*?(?=\\'))|(((?<=token: )|(?<=token description: )).*))', line, re.M|re.I)\n",
    "    num = re.search(r'((?<=\\' on \\').*?(?=\\')|(?<=# Requested \\').*?(?=\\'))', line, re.M|re.I)\n",
    "    surl = re.search(r'(((?<=SURL\\(s\\): \\').*?((?=\\')|(.*)))|((?<=surl\\(s\\): ).*?(?= token))|((?<=surl\\(s\\): ).*))', line, re.M|re.I)\n",
    "    result = re.search(r'(((?<=is \\').*?(?=\\'))|((?<=__process_file_request\\<\\> : ).*))', line, re.M|re.I)\n",
    "    \n",
    "    if re.search(r'((logConfiguration : )|(initSoap : )|(main : ))', line, re.M|re.I):\n",
    "        startup=line\n",
    "    if re.search(r'(rpcResponseHandler_AbortFiles)', line, re.M|re.I):\n",
    "        ResponseHandler=line\n",
    "    \n",
    "    result={'Request':request,'DN':DN,'requested_token':token,'num':num,'surl':surl,'result':result, 'ip':ip, 'startup':startup}\n",
    "    for key, value in result.iteritems():\n",
    "        if key=='startup':\n",
    "            result[key]=startup\n",
    "            continue\n",
    "        if key=='result' and ResponseHandler != '':\n",
    "            result[key]=ResponseHandler\n",
    "            continue\n",
    "        if value:\n",
    "            result[key]=value.group()\n",
    "            if key=='surl':\n",
    "                result[key]=result[key].replace(\"\\'\",\"\")\n",
    "                result[key]=result[key].split()\n",
    "        else:\n",
    "            result[key]=''\n",
    "        \n",
    "    if len(result['surl'])>4:\n",
    "        result['surl'][3]=result['surl'][4]\n",
    "        result['surl']=result['surl'][:4]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_parser(storm_df):\n",
    "    \"\"\"\n",
    "    Recive stoRM dataframe with message NOT parsed ad use funzb to return a new DF with parsed message.\n",
    "    Takes about 15 min to parse 4mln lines log. \n",
    "    \n",
    "    \"\"\"\n",
    "    inizio = {'DN': '', 'Request': '', 'ip': '', 'num': '', 'requested_token': '', 'result': '', 'startup': '', 'surl': ''}\n",
    "    lista = list(storm_df['message'])\n",
    "    listaa = map(funzb,lista)\n",
    "    finale={}\n",
    "    for key in inizio.iterkeys():\n",
    "        finale[key]=list(finale[key] for finale in listaa)\n",
    "    finale = pd.DataFrame.from_dict(finale)\n",
    "    finalone= pd.concat([storm_df, finale], axis=1)\n",
    "    return finalone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.6 s, sys: 1.3 s, total: 23.9 s\n",
      "Wall time: 24.9 s\n"
     ]
    }
   ],
   "source": [
    "%time DF = dframer('stoRM_FE_kibana.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date ISO-8601', 'timestamp', 'type', 'thread', 'request_id', 'message']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4585705"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_parser(DF).to_csv('stoRM_FE_kibana_parsed'+ '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date ISO-8601', 'timestamp', 'type', 'thread', 'request_id', 'message']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inizio={'DN': '', 'Request': '', 'ip': '', 'num': '', 'requested_token': '', 'result': '', 'startup': '', 'surl': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 53s, sys: 5.52 s, total: 15min 59s\n",
      "Wall time: 16min 3s\n"
     ]
    }
   ],
   "source": [
    "%time listaa = map(funzb,lista)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interessante, utilizzando map il tempo che ci mette il sistema ad applicare la funzb a tutti i messaggi Ã¨ di 16min per 4585705 righe di log. Sfruttando quindi map potremmo riuscire ad accellerare e velocizzare il parsing di tutti questi message. Rimane da capire come una volta che abbiamo i mille dizionari possiamo riunirli al DF iniziale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_parser_compressed(storm_df):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    inizio = {'DN': '', 'Request': '', 'ip': '', 'num': '', 'requested_token': '', 'result': '', 'startup': '', 'surl': ''}\n",
    "    finale={}\n",
    "    for key in inizio.iterkeys():\n",
    "        finale[key]=list(finale[key] for finale in map(funzb,list(storm_df['message'])))\n",
    "    return pd.concat([storm_df, pd.DataFrame.from_dict(finale)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
